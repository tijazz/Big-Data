{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Hadoop_HDFS_Exercise1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNpuJpWGj5VnjByBPRf57wA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tijazz/Big-Data/blob/main/Hadoop_HDFS_Exercise1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise\n",
        "Create a new colab notebook then:\n",
        "\n",
        "1.   Download Hadoop\n",
        "2.   Set Java_Home\n",
        "3.   Create a new folder called 'hello_hadoop'\n",
        "4.   download the following file 'https://raw.githubusercontent.com/besherh/BigDataManagement/main/word_count.txt' and move it to 'hellp_hadoop' directory\n",
        "5.  apply map reduce to count the words in the previous file\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "82H1c599McVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hadoop Installation on my Colab Notebook\n"
      ],
      "metadata": {
        "id": "nZtHEJslY9hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
        "!wget https://raw.githubusercontent.com/besherh/BigDataManagement/924228b1a3fec29b6240e0de5e893cf7493a7de4/hadoop-examples.jar.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwkAFHbVYrp7",
        "outputId": "5ce10b91-1370-478f-ef7d-c969d5a3a97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-06 18:54:45--  https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.95.219, 2a01:4f8:10a:201a::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 500749234 (478M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.0.tar.gz’\n",
            "\n",
            "hadoop-3.3.0.tar.gz 100%[===================>] 477.55M  23.5MB/s    in 21s     \n",
            "\n",
            "2022-02-06 18:55:07 (22.3 MB/s) - ‘hadoop-3.3.0.tar.gz’ saved [500749234/500749234]\n",
            "\n",
            "--2022-02-06 18:55:07--  https://raw.githubusercontent.com/besherh/BigDataManagement/924228b1a3fec29b6240e0de5e893cf7493a7de4/hadoop-examples.jar.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 129392 (126K) [application/zip]\n",
            "Saving to: ‘hadoop-examples.jar.zip’\n",
            "\n",
            "hadoop-examples.jar 100%[===================>] 126.36K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-02-06 18:55:08 (5.88 MB/s) - ‘hadoop-examples.jar.zip’ saved [129392/129392]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we’ll use the tar command with the -x flag to extract, -z to uncompress, -v for verbose output, and -f to specify that we’re extracting from a file\n",
        "\n"
      ],
      "metadata": {
        "id": "gVHfrQS3ouID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf hadoop-3.3.0.tar.gz"
      ],
      "metadata": {
        "id": "-hlAjvYYZGvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#copy  hadoop file to user/local\n",
        "!cp -r hadoop-3.3.0/ /usr/local/\n"
      ],
      "metadata": {
        "id": "5eH4Wm-lY1dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unzip the archive file (jars)\n",
        "!unzip hadoop-examples.jar.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkj3laVHZhz7",
        "outputId": "3413d8d9-192c-4672-c971-9a8b5956e7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  hadoop-examples.jar.zip\n",
            "  inflating: hadoop-examples.jar     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#house cleaning i.e removing the installation files\n",
        "!rm -r hadoop-3.3.0/\n",
        "!rm  hadoop-3.3.0.tar.gz\n",
        "!rm  hadoop-examples.jar.zip\n"
      ],
      "metadata": {
        "id": "7Ls3lv0CZos7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configuring and Setting Hadoop’s Java Home\n",
        "Hadoop requires that you set the path to Java, either as an environment variable or in the Hadoop configuration file.\n",
        "\n"
      ],
      "metadata": {
        "id": "DAoLDnHLq0jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To find the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYp7rRL4Z-II",
        "outputId": "85cb0ffa-170b-4317-8f8c-d1ad1b7b9885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make sure to add the same path that you get from the previous command\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64/\"\n",
        "\n"
      ],
      "metadata": {
        "id": "lVp7AGUtru3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.0/bin/hadoop"
      ],
      "metadata": {
        "id": "w5wXTa3gzS2B",
        "outputId": "4682c6c3-772c-48c7-dcde-7b3ccdd2182f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: Create a new folder called 'hello_hadoop'"
      ],
      "metadata": {
        "id": "UmrKNAQ7afk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a new directory \n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop fs -mkdir hello_hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwf3MXYtajMC",
        "outputId": "b79b58c2-d8b5-4ea3-89f2-54c66b1f2c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: `hello_hadoop': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check if directory exists in the folder\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop fs -ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6ADimVLbri5",
        "outputId": "49f763be-cca9-42cf-929e-d4a290b5ebd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 items\n",
            "drwxr-xr-x   - root root       4096 2022-02-01 14:31 .config\n",
            "-rw-r--r--   1 root root     142466 2013-10-16 16:52 hadoop-examples.jar\n",
            "drwxr-xr-x   - root root       4096 2022-02-06 19:06 hello_hadoop\n",
            "drwxr-xr-x   - root root       4096 2022-02-01 14:32 sample_data\n",
            "-rw-r--r--   1 root root       4964 2022-02-06 19:21 word_count.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Download the following file 'https://raw.githubusercontent.com/besherh/BigDataManagement/main/word_count.txt' and move it to 'hello_hadoop' directory"
      ],
      "metadata": {
        "id": "p9ocEE9pcDPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/besherh/BigDataManagement/main/word_count.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEU3JG-FcIhG",
        "outputId": "c9aba41a-aac8-4ec1-8573-214bed692bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-06 19:22:30--  https://raw.githubusercontent.com/besherh/BigDataManagement/main/word_count.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4964 (4.8K) [text/plain]\n",
            "Saving to: ‘word_count.txt.1’\n",
            "\n",
            "\rword_count.txt.1      0%[                    ]       0  --.-KB/s               \rword_count.txt.1    100%[===================>]   4.85K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-06 19:22:30 (35.9 MB/s) - ‘word_count.txt.1’ saved [4964/4964]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check if directory exists in the folder\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop fs -ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_4ThPgEblok",
        "outputId": "97ef2d35-24f4-40bc-8e31-58b4824e6482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 items\n",
            "drwxr-xr-x   - root root       4096 2022-02-01 14:31 .config\n",
            "-rw-r--r--   1 root root     142466 2013-10-16 16:52 hadoop-examples.jar\n",
            "drwxr-xr-x   - root root       4096 2022-02-06 19:06 hello_hadoop\n",
            "drwxr-xr-x   - root root       4096 2022-02-01 14:32 sample_data\n",
            "-rw-r--r--   1 root root       4964 2022-02-06 19:21 word_count.txt\n",
            "-rw-r--r--   1 root root       4964 2022-02-06 19:22 word_count.txt.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Move the downloaded file into hello_hadoop folder\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop fs -mv word_count.txt /usr/local/"
      ],
      "metadata": {
        "id": "jIrRi4nDchl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if directory exists in the previous folder\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop fs -ls /usr/local/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1dMw59OddAk",
        "outputId": "d281ac52-6131-4b50-fe59-c7734d0e809e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22 items\n",
            "-rw-r--r--   1 root root       1636 2022-02-03 14:26 /usr/local/LICENSE.txt\n",
            "drwxr-xr-x   - root root       4096 2022-02-03 14:22 /usr/local/_gcs_config_ops.so\n",
            "drwxr-xr-x   - root root       4096 2022-02-04 14:12 /usr/local/bin\n",
            "drwxr-xr-x   - root root       4096 2022-02-01 14:24 /usr/local/cuda\n",
            "drwxr-xr-x   - root root       4096 2022-02-01 14:17 /usr/local/cuda-10.0\n",
            "drwxr-xr-x   - root root       4096 2022-02-01 14:20 /usr/local/cuda-10.1\n",
            "drwxr-xr-x   - root root       4096 2022-02-01 14:24 /usr/local/cuda-11\n",
            "drwxr-xr-x   - root root       4096 2022-02-01 14:22 /usr/local/cuda-11.0\n",
            "drwxr-xr-x   - root root       4096 2022-02-01 14:24 /usr/local/cuda-11.1\n",
            "drwxr-xr-x   - root root       4096 2022-02-03 14:11 /usr/local/etc\n",
            "drwxr-xr-x   - root root       4096 2020-11-19 13:07 /usr/local/games\n",
            "drwxr-xr-x   - root root       4096 2022-02-06 18:57 /usr/local/hadoop-3.3.0\n",
            "drwxr-xr-x   - root root       4096 2022-02-03 14:30 /usr/local/include\n",
            "drwxr-xr-x   - root root       4096 2022-02-03 14:30 /usr/local/lib\n",
            "drwxr-xr-x   - root root       4096 2022-02-03 14:22 /usr/local/licensing\n",
            "drwxr-xr-x   - root root       4096 2022-02-03 14:20 /usr/local/man\n",
            "drwxr-xr-x   - root root       4096 2020-11-19 13:09 /usr/local/sbin\n",
            "-rw-r--r--   1 root root       7291 2022-02-03 14:26 /usr/local/setup.cfg\n",
            "drwxr-xr-x   - root root       4096 2022-02-03 14:20 /usr/local/share\n",
            "drwxr-xr-x   - root root       4096 2020-11-19 13:07 /usr/local/src\n",
            "-rw-r--r--   1 root root       4964 2022-02-06 19:21 /usr/local/word_count.txt\n",
            "drwxr-xr-x   - root root       4096 2022-02-03 14:22 /usr/local/xgboost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Move the downloaded file into hello_hadoop folder\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop fs -mv /usr/local/word_count.txt ./hello_hadoop"
      ],
      "metadata": {
        "id": "PuAN8xvkgC1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if directory exists in the folder\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop fs -ls ./hello_hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXvEb0ZGdjec",
        "outputId": "f2eb9352-f4aa-4502-a2f1-d1ba8aec477b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root root       4964 2022-02-06 19:21 hello_hadoop/word_count.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5 : Apply map reduce to count the words in the previous file"
      ],
      "metadata": {
        "id": "F5wZhWlHgsJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the word count function from mapReduce\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop jar ./hadoop-examples.jar wordcount\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjgc4MS-gvjl",
        "outputId": "e548e82a-92cb-455b-d967-49c9fb3c7cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: wordcount <in> [<in>...] <out>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#running the wordcount function on our text file\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop jar ./hadoop-examples.jar wordcount ./hello_hadoop/word_count.txt out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9lawECnhp68",
        "outputId": "8771d2f0-7ca3-4323-f68a-9c4753509b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-06 19:35:04,070 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-02-06 19:35:04,260 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-02-06 19:35:04,260 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/content/out already exists\n",
            "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:164)\n",
            "\tat org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:277)\n",
            "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)\n",
            "\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1576)\n",
            "\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1573)\n",
            "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
            "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
            "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n",
            "\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1573)\n",
            "\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1594)\n",
            "\tat org.apache.hadoop.examples.WordCount.main(WordCount.java:87)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n",
            "\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n",
            "\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:323)\n",
            "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:236)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's Look inside output directory for our wordcount output\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop fs -ls ./out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beJBrmChiHXP",
        "outputId": "3ef9cc67-cef2-42d1-d833-1a141751d515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2022-02-06 19:34 out/_SUCCESS\n",
            "-rw-r--r--   1 root root       3643 2022-02-06 19:34 out/part-r-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The Cat command will help read our output to the screen and suitable for files with small data.\n",
        "! cat out/part-r-00000 local.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4XRjX25iyyF",
        "outputId": "42061897-a81e-4e61-f45d-48e9a5edc036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\t1\n",
            "America\t1\n",
            "American\t1\n",
            "As\t2\n",
            "But\t2\n",
            "Classification\t1\n",
            "Contents\t1\n",
            "Details\t2\n",
            "Different\t1\n",
            "External\t1\n",
            "Fantasy\t1\n",
            "Fiction\t1\n",
            "However\t3\n",
            "Hyphenated\t1\n",
            "In\t4\n",
            "Jane\t1\n",
            "JavaScript\t1\n",
            "Modern\t1\n",
            "Month\t1\n",
            "Most\t2\n",
            "National\t1\n",
            "Nebula\t1\n",
            "Novel\t2\n",
            "Novelette\t1\n",
            "Novelist\t1\n",
            "Novella\t1\n",
            "Numerous\t1\n",
            "Ph.D.\t1\n",
            "Please\t1\n",
            "References\t1\n",
            "Science\t1\n",
            "See\t1\n",
            "Short\t1\n",
            "Smiley\t2\n",
            "Software\t2\n",
            "Sources\t1\n",
            "The\t5\n",
            "There\t1\n",
            "These\t1\n",
            "This\t2\n",
            "To\t1\n",
            "Unix\t1\n",
            "Unsourced\t1\n",
            "Usually\t1\n",
            "Variations\t1\n",
            "When\t1\n",
            "Wikipedia\t1\n",
            "Word\t4\n",
            "Writers\t1\n",
            "Writing\t1\n",
            "a\t28\n",
            "about\t1\n",
            "abstracts\t1\n",
            "academia\t1\n",
            "academic\t1\n",
            "accept\t1\n",
            "acceptable\t2\n",
            "accordingly\t1\n",
            "across\t1\n",
            "adding\t1\n",
            "adjective\t1\n",
            "advent\t1\n",
            "advertising\t1\n",
            "already\t1\n",
            "also\t5\n",
            "an\t4\n",
            "and\t23\n",
            "any\t2\n",
            "application\t1\n",
            "applications\t2\n",
            "arbiter\t1\n",
            "arbitrary\t1\n",
            "are\t4\n",
            "articles\t1\n",
            "as\t9\n",
            "assignments\t1\n",
            "at\t3\n",
            "automatically\t1\n",
            "average\t1\n",
            "award\t1\n",
            "barring\t1\n",
            "be\t8\n",
            "because\t2\n",
            "behavior\t1\n",
            "being\t1\n",
            "between\t2\n",
            "bibliographies\t1\n",
            "bookmarklet\t1\n",
            "books\t1\n",
            "bottom\t1\n",
            "boundaries\t1\n",
            "boundary\t1\n",
            "broad\t1\n",
            "broadly\t1\n",
            "browsers\t1\n",
            "but\t2\n",
            "by\t5\n",
            "calculate\t1\n",
            "can\t5\n",
            "captions\t1\n",
            "case\t1\n",
            "categories\t1\n",
            "categorise\t1\n",
            "category\t1\n",
            "certain\t2\n",
            "challenged\t1\n",
            "chapter\t1\n",
            "character\t2\n",
            "characters\t2\n",
            "charge\t1\n",
            "children\t1\n",
            "choice\t1\n",
            "citations\t1\n",
            "cite\t1\n",
            "client\t1\n",
            "commonly\t1\n",
            "compounds\t1\n",
            "conjunctions\t1\n",
            "consensus\t3\n",
            "consistent\t1\n",
            "converting\t1\n",
            "costless\t1\n",
            "could\t1\n",
            "count\t11\n",
            "counted\t2\n",
            "counting\t6\n",
            "counts\t3\n",
            "define\t1\n",
            "defined\t1\n",
            "definition\t3\n",
            "definitions\t3\n",
            "dependent\t1\n",
            "depending\t2\n",
            "depends\t1\n",
            "details\t2\n",
            "determine\t1\n",
            "determined\t1\n",
            "differed\t1\n",
            "different\t1\n",
            "difficult\t1\n",
            "dissertation\t1\n",
            "dissertations\t1\n",
            "divider\t1\n",
            "dividers\t1\n",
            "document\t2\n",
            "documents\t1\n",
            "does\t1\n",
            "don't\t2\n",
            "done\t1\n",
            "drilled\t1\n",
            "during\t1\n",
            "each\t1\n",
            "earlier\t2\n",
            "effort\t1\n",
            "effortless\t1\n",
            "elements\t1\n",
            "em\t1\n",
            "encyclopedia\t1\n",
            "endnotes\t2\n",
            "era\t2\n",
            "especially\t1\n",
            "exact\t1\n",
            "example\t2\n",
            "exceeding\t1\n",
            "exclude\t1\n",
            "explained\t1\n",
            "extensions\t1\n",
            "extent\t1\n",
            "fallen\t1\n",
            "fast\t1\n",
            "feature\t1\n",
            "fiction\t4\n",
            "figure\t1\n",
            "firm\t1\n",
            "first\t1\n",
            "five\t1\n",
            "follow\t1\n",
            "follows\t1\n",
            "footnotes\t2\n",
            "for\t10\n",
            "found\t1\n",
            "free\t1\n",
            "from\t2\n",
            "generally\t2\n",
            "give\t2\n",
            "greatly\t1\n",
            "handwriting\t1\n",
            "have\t1\n",
            "help\t1\n",
            "hence\t1\n",
            "hidden\t2\n",
            "hosted\t1\n",
            "how\t3\n",
            "hyphen\t1\n",
            "important\t1\n",
            "importantly\t1\n",
            "improve\t1\n",
            "in\t11\n",
            "include\t1\n",
            "included\t1\n",
            "including\t1\n",
            "integer\t1\n",
            "is\t19\n",
            "it\t2\n",
            "its\t3\n",
            "job\t1\n",
            "journalism\t1\n",
            "large\t1\n",
            "largely\t1\n",
            "least\t1\n",
            "legal\t1\n",
            "length\t5\n",
            "lengths\t2\n",
            "like\t1\n",
            "limit\t2\n",
            "line\t3\n",
            "links\t1\n",
            "lists\t2\n",
            "literary\t1\n",
            "long\t1\n",
            "main\t1\n",
            "major\t1\n",
            "many\t2\n",
            "material\t1\n",
            "may\t8\n",
            "measure\t2\n",
            "measures\t1\n",
            "mentioned\t1\n",
            "might\t1\n",
            "minute\t1\n",
            "most\t3\n",
            "mystery\t1\n",
            "namely\t2\n",
            "needed\t1\n",
            "no\t1\n",
            "non\t2\n",
            "not\t2\n",
            "noun\t1\n",
            "novel\t4\n",
            "novella\t1\n",
            "novels\t3\n",
            "now\t1\n",
            "number\t3\n",
            "numbers\t1\n",
            "occur\t1\n",
            "of\t25\n",
            "often\t3\n",
            "on\t7\n",
            "one\t1\n",
            "operational\t2\n",
            "or\t11\n",
            "others\t1\n",
            "outside\t1\n",
            "over\t2\n",
            "particularly\t1\n",
            "passage\t1\n",
            "per\t3\n",
            "permanent\t1\n",
            "permission\t1\n",
            "person\t1\n",
            "predominantly\t1\n",
            "prepositions\t1\n",
            "price\t1\n",
            "proceedings\t1\n",
            "processing\t4\n",
            "processors\t1\n",
            "program\t1\n",
            "programs\t2\n",
            "quality\t1\n",
            "range\t1\n",
            "readability\t1\n",
            "reading\t1\n",
            "reference\t1\n",
            "references\t1\n",
            "regular\t1\n",
            "reliable\t1\n",
            "removed\t1\n",
            "required\t1\n",
            "requirement\t1\n",
            "requires\t1\n",
            "result\t1\n",
            "results\t2\n",
            "rule\t4\n",
            "rules\t5\n",
            "save\t1\n",
            "school\t1\n",
            "script\t1\n",
            "section\t2\n",
            "sections\t1\n",
            "segmentation\t4\n",
            "select\t1\n",
            "similar\t1\n",
            "simply\t1\n",
            "since\t1\n",
            "six\t1\n",
            "slash\t1\n",
            "software\t2\n",
            "some\t1\n",
            "sometimes\t1\n",
            "sources\t2\n",
            "space\t3\n",
            "special\t1\n",
            "specifically\t1\n",
            "specifies\t1\n",
            "speeds\t1\n",
            "standard\t1\n",
            "start\t1\n",
            "stay\t1\n",
            "story\t1\n",
            "strict\t1\n",
            "students\t2\n",
            "subgenre\t1\n",
            "subject\t1\n",
            "such\t7\n",
            "suggests\t1\n",
            "support\t1\n",
            "systems\t1\n",
            "tab\t1\n",
            "tables\t1\n",
            "teacher\t1\n",
            "term\t1\n",
            "text\t8\n",
            "text)\t1\n",
            "that\t5\n",
            "the\t38\n",
            "their\t1\n",
            "there\t1\n",
            "these\t2\n",
            "this\t2\n",
            "thriller\t1\n",
            "thumb\t1\n",
            "thus\t1\n",
            "time\t1\n",
            "to\t18\n",
            "todays\t1\n",
            "too\t1\n",
            "total\t2\n",
            "toward\t2\n",
            "trait\t1\n",
            "translation\t1\n",
            "translators\t1\n",
            "tremendously\t1\n",
            "typewriters\t1\n",
            "typical\t1\n",
            "typically\t1\n",
            "typing\t1\n",
            "under\t1\n",
            "universities\t1\n",
            "up\t2\n",
            "update\t1\n",
            "used\t4\n",
            "users\t1\n",
            "usually\t3\n",
            "variation\t1\n",
            "variations\t2\n",
            "varies\t1\n",
            "various\t1\n",
            "vary\t2\n",
            "varying\t2\n",
            "via\t2\n",
            "was\t1\n",
            "watch\t1\n",
            "wayside\t1\n",
            "wc\t1\n",
            "web\t1\n",
            "website\t1\n",
            "were\t3\n",
            "what\t1\n",
            "when\t2\n",
            "whether\t1\n",
            "which\t4\n",
            "while\t3\n",
            "whitespace\t1\n",
            "widespread\t1\n",
            "with\t2\n",
            "within\t1\n",
            "word\t24\n",
            "words\t21\n",
            "work\t1\n",
            "workers\t1\n",
            "writer\t1\n",
            "cat: local.txt: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.0/bin/hadoop fs -ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTGTo0lvkTC2",
        "outputId": "09bfffdc-f9b5-46e6-aaaf-8a2aa27a046c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 items\n",
            "drwxr-xr-x   - root root       4096 2022-02-01 14:31 .config\n",
            "-rw-r--r--   1 root root     142466 2013-10-16 16:52 hadoop-examples.jar\n",
            "drwxr-xr-x   - root root       4096 2022-02-06 19:27 hello_hadoop\n",
            "drwxr-xr-x   - root root       4096 2022-02-06 19:34 out\n",
            "drwxr-xr-x   - root root       4096 2022-02-01 14:32 sample_data\n",
            "-rw-r--r--   1 root root       4964 2022-02-06 19:22 word_count.txt.1\n"
          ]
        }
      ]
    }
  ]
}